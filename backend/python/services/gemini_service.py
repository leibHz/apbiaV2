"""
Servi√ßo de integra√ß√£o com Google Gemini AI
"""
import google.generativeai as genai
from typing import Optional, List, Dict, Tuple
from config.settings import settings
from utils.logger import logger
import time


class GeminiService:
    """Servi√ßo para intera√ß√£o com o Gemini AI"""
    
    def __init__(self):
        # Configura API do Gemini
        genai.configure(api_key=settings.GOOGLE_API_KEY)
        
        # Configura√ß√µes de gera√ß√£o
        self.generation_config = {
            "temperature": 0.7,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
        }
        
        # Configura√ß√µes de seguran√ßa
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]
        
        # Inicializa modelo
        self.model = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Inicializa o modelo Gemini"""
        try:
            # Configura√ß√£o espec√≠fica para thinking mode
            model_config = {
                "generation_config": self.generation_config,
                "safety_settings": self.safety_settings
            }
            
            # Se thinking mode estiver ativado, adiciona nas instru√ß√µes do sistema
            system_instruction = self._get_system_instruction()
            
            self.model = genai.GenerativeModel(
                model_name=settings.GEMINI_MODEL,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings,
                system_instruction=system_instruction
            )
            
            logger.info(f"‚úÖ Modelo Gemini inicializado: {settings.GEMINI_MODEL}")
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar modelo Gemini: {e}")
            raise
    
    def _get_system_instruction(self) -> str:
        """Retorna as instru√ß√µes do sistema para a IA"""
        instruction = """Voc√™ √© o APBIA (Ajudante de Projetos para Bragantec Baseado em IA), 
um assistente especializado em auxiliar estudantes do IFSP Bragan√ßa Paulista na feira de ci√™ncias Bragantec.

Sua fun√ß√£o √©:
1. Ajudar os estudantes a desenvolver projetos cient√≠ficos de qualidade
2. Fornecer ideias criativas e inovadoras para projetos
3. Auxiliar no planejamento e organiza√ß√£o dos projetos
4. Responder d√∫vidas sobre metodologia cient√≠fica
5. Dar feedback construtivo sobre ideias e propostas

Voc√™ tem acesso aos cadernos de resumos das edi√ß√µes anteriores da Bragantec como contexto.

Seja sempre:
- Did√°tico e claro em suas explica√ß√µes
- Encorajador e positivo
- Cient√≠fico, mas acess√≠vel
- Criativo ao sugerir ideias
- √âtico e respons√°vel

Se voc√™ receber uma pergunta complexa que requer reflex√£o profunda, use seu modo de pensamento 
para analisar cuidadosamente antes de responder."""

        if settings.GEMINI_THINKING_MODE:
            instruction += "\n\nPara perguntas complexas, pense cuidadosamente antes de responder."
        
        return instruction
    
    def gerar_resposta(self, mensagem: str, contexto: Optional[List[str]] = None, 
                      historico: Optional[List[Dict]] = None) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        Gera resposta usando o Gemini
        
        Args:
            mensagem: Mensagem do usu√°rio
            contexto: Lista de textos de contexto (arquivos TXT da Bragantec)
            historico: Hist√≥rico de conversas anterior
        
        Returns:
            Tuple[success, resposta, error_message]
        """
        try:
            # Inicia chat
            if historico:
                chat = self.model.start_chat(history=self._format_historico(historico))
            else:
                chat = self.model.start_chat()
            
            # Prepara prompt com contexto
            prompt = self._build_prompt(mensagem, contexto)
            
            # Gera resposta
            logger.info(f"üì§ Enviando mensagem para Gemini...")
            response = chat.send_message(prompt)
            
            # Extrai texto da resposta
            resposta_texto = response.text
            
            logger.info(f"‚úÖ Resposta recebida do Gemini")
            logger.log_api_call("Gemini", self._estimate_tokens(prompt + resposta_texto))
            
            return True, resposta_texto, None
            
        except Exception as e:
            error_msg = f"Erro ao gerar resposta: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            return False, None, error_msg
    
    def gerar_resposta_com_thinking(self, mensagem: str, contexto: Optional[List[str]] = None) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        Gera resposta usando thinking mode para perguntas complexas
        
        Returns:
            Tuple[success, resposta, error_message]
        """
        try:
            # Adiciona instru√ß√£o para pensar
            prompt_thinking = f"""Esta √© uma pergunta que requer reflex√£o cuidadosa.
Por favor, pense profundamente sobre ela antes de responder.

Pergunta: {mensagem}"""
            
            if contexto:
                prompt_thinking += f"\n\nContexto dispon√≠vel:\n{self._format_contexto(contexto)}"
            
            # Gera resposta com thinking
            response = self.model.generate_content(
                prompt_thinking,
                generation_config={
                    **self.generation_config,
                    "temperature": 0.9  # Aumenta criatividade para thinking
                }
            )
            
            resposta_texto = response.text
            
            logger.info(f"‚úÖ Resposta com thinking mode gerada")
            return True, resposta_texto, None
            
        except Exception as e:
            error_msg = f"Erro ao gerar resposta com thinking: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            return False, None, error_msg
    
    def _build_prompt(self, mensagem: str, contexto: Optional[List[str]] = None) -> str:
        """Constr√≥i o prompt com mensagem e contexto"""
        prompt = ""
        
        if contexto:
            prompt += "=== CONTEXTO (Cadernos de Resumos da Bragantec) ===\n"
            prompt += self._format_contexto(contexto)
            prompt += "\n=== FIM DO CONTEXTO ===\n\n"
        
        prompt += f"Pergunta do estudante: {mensagem}"
        
        return prompt
    
    def _format_contexto(self, contexto: List[str]) -> str:
        """Formata lista de contextos em texto √∫nico"""
        return "\n\n---\n\n".join(contexto)
    
    def _format_historico(self, historico: List[Dict]) -> List[Dict]:
        """Formata hist√≥rico para o formato do Gemini"""
        formatted = []
        
        for msg in historico:
            role = "user" if msg.get("usuario_id") else "model"
            formatted.append({
                "role": role,
                "parts": [msg.get("conteudo", "")]
            })
        
        return formatted
    
    def _estimate_tokens(self, text: str) -> int:
        """Estima n√∫mero de tokens (aproxima√ß√£o)"""
        # Aproxima√ß√£o: ~4 caracteres por token
        return len(text) // 4
    
    def upload_arquivo_para_gemini(self, file_path: str, file_content: bytes, mime_type: str) -> Tuple[bool, Optional[any], Optional[str]]:
        """
        Faz upload de arquivo diretamente para o Gemini (para processamento de documentos)
        
        Returns:
            Tuple[success, file_object, error_message]
        """
        try:
            # Salva arquivo temporariamente
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file_path).suffix) as tmp:
                tmp.write(file_content)
                tmp_path = tmp.name
            
            # Upload para Gemini
            file = genai.upload_file(tmp_path, mime_type=mime_type)
            
            # Aguarda processamento
            while file.state.name == "PROCESSING":
                time.sleep(2)
                file = genai.get_file(file.name)
            
            if file.state.name == "FAILED":
                raise Exception("Falha ao processar arquivo")
            
            logger.info(f"‚úÖ Arquivo enviado para Gemini: {file_path}")
            return True, file, None
            
        except Exception as e:
            error_msg = f"Erro ao enviar arquivo para Gemini: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            return False, None, error_msg
    
    def processar_documento(self, file_object: any, pergunta: str) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        Processa documento com o Gemini
        
        Returns:
            Tuple[success, resposta, error_message]
        """
        try:
            prompt = [file_object, pergunta]
            response = self.model.generate_content(prompt)
            
            logger.info(f"‚úÖ Documento processado")
            return True, response.text, None
            
        except Exception as e:
            error_msg = f"Erro ao processar documento: {str(e)}"
            logger.error(f"‚ùå {error_msg}")
            return False, None, error_msg


# Inst√¢ncia global
gemini_service = GeminiService()